{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TV-GCNN.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "CkNFgwo2YbNG",
        "colab_type": "code",
        "outputId": "65c973c6-6bb7-4086-89eb-3518d4822d00",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 436
        }
      },
      "source": [
        "!pip install pygsp\n",
        "!pip install ndlib"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pygsp in /usr/local/lib/python3.6/dist-packages (0.5.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from pygsp) (1.3.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from pygsp) (1.16.5)\n",
            "Requirement already satisfied: ndlib in /usr/local/lib/python3.6/dist-packages (5.0.0)\n",
            "Requirement already satisfied: dynetx in /usr/local/lib/python3.6/dist-packages (from ndlib) (0.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from ndlib) (1.3.1)\n",
            "Requirement already satisfied: bokeh in /usr/local/lib/python3.6/dist-packages (from ndlib) (1.0.4)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.6/dist-packages (from ndlib) (2.3)\n",
            "Requirement already satisfied: python-igraph in /usr/local/lib/python3.6/dist-packages (from ndlib) (0.7.1.post6)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from ndlib) (1.16.5)\n",
            "Requirement already satisfied: netdispatch in /usr/local/lib/python3.6/dist-packages (from ndlib) (0.0.3)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from ndlib) (0.16.0)\n",
            "Requirement already satisfied: Jinja2>=2.7 in /usr/local/lib/python3.6/dist-packages (from bokeh->ndlib) (2.10.3)\n",
            "Requirement already satisfied: pillow>=4.0 in /usr/local/lib/python3.6/dist-packages (from bokeh->ndlib) (4.3.0)\n",
            "Requirement already satisfied: tornado>=4.3 in /usr/local/lib/python3.6/dist-packages (from bokeh->ndlib) (4.5.3)\n",
            "Requirement already satisfied: packaging>=16.8 in /usr/local/lib/python3.6/dist-packages (from bokeh->ndlib) (19.2)\n",
            "Requirement already satisfied: PyYAML>=3.10 in /usr/local/lib/python3.6/dist-packages (from bokeh->ndlib) (3.13)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from bokeh->ndlib) (2.5.3)\n",
            "Requirement already satisfied: six>=1.5.2 in /usr/local/lib/python3.6/dist-packages (from bokeh->ndlib) (1.12.0)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.6/dist-packages (from networkx->ndlib) (4.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.6/dist-packages (from Jinja2>=2.7->bokeh->ndlib) (1.1.1)\n",
            "Requirement already satisfied: olefile in /usr/local/lib/python3.6/dist-packages (from pillow>=4.0->bokeh->ndlib) (0.46)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging>=16.8->bokeh->ndlib) (2.4.2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sO6lWJfmXjrj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import time\n",
        "import os\n",
        "import argparse\n",
        "import sys\n",
        "import numpy as np\n",
        "import json\n",
        "import matplotlib\n",
        "import random\n",
        "\n",
        "#Take off warnings\n",
        "import warnings\n",
        "def fxn():\n",
        "    warnings.warn(\"deprecated\", DeprecationWarning)\n",
        "with warnings.catch_warnings():\n",
        "    warnings.simplefilter(\"ignore\")\n",
        "    fxn()\n",
        "\n",
        "\n",
        "\n",
        "from minibatch_sources import MinibatchSource\n",
        "\n",
        "matplotlib.use(\"Agg\")\n",
        "\n",
        "from pygsp import graphs\n",
        "import networkx as nx\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "from models import deep_fir_tv_fc_fn, fc_fn, \\\n",
        "    deep_cheb_fc_fn, deep_sep_fir_fc_fn\n",
        "from laplacian import initialize_laplacian_tensor\n",
        "from coarsening import coarsen, perm_data, keep_pooling_laplacians\n",
        "from data_epidemics import epidemics_generator, load_from_npy, save_to_npy\n",
        "\n",
        "#from graph_utils.visualization import plot_tf_fir_filter\n",
        "\n",
        "FLAGS = None\n",
        "#FILEDIR = os.path.dirname(os.path.realpath(__file__))\n",
        "#TEMPDIR = os.path.realpath(os.path.join(FILEDIR, \"../experiments\"))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QLrSt6iZh4WL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_type=\"deep_fir\"\n",
        "action=\"train\"\n",
        "learning_rate=1e-4\n",
        "num_epochs=1\n",
        "num_train=6000\n",
        "num_test=2000\n",
        "batch_size=100\n",
        "seed=15\n",
        "shot_noise=1\n",
        "num_vertices=500\n",
        "num_frames=128\n",
        "vertex_filter_orders=[4, 4, 4]\n",
        "time_filter_orders=[3, 3, 3]\n",
        "num_filters=[8, 16, 32]\n",
        "time_poolings=[4, 4, 4]\n",
        "vertex_poolings=[2, 2, 2]\n",
        "num_classes=2\n",
        "load_data=False"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f4P6WsO9XlQE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def _fill_feed_dict(mb_source, x, y, dropout, phase, is_training):\n",
        "    (data, labels), is_end = mb_source.next_batch(batch_size)\n",
        "    feed_dict = {x: data, y: labels, dropout: 0.5 if is_training else 1, phase: is_training}\n",
        "    still_data = not is_end\n",
        "    return feed_dict, still_data\n",
        "\n",
        "\n",
        "def run_training(L, train_mb_source, test_mb_source):\n",
        "    \"\"\"Performs training and evaluation.\"\"\"\n",
        "\n",
        "    # Create data placeholders\n",
        "    num_vertices, _ = L[0].get_shape()\n",
        "    x = tf.compat.v1.placeholder(tf.float32, [None, num_vertices, num_frames, 1], name=\"x\")\n",
        "    y_ = tf.compat.v1.placeholder(tf.uint8, name=\"labels\")\n",
        "    y_hot = tf.one_hot(y_, num_classes)\n",
        "\n",
        "    # Initialize model\n",
        "    if model_type == \"deep_fir\":\n",
        "        print(\"Training deep FIR-TV model...\")\n",
        "        logits, phase = deep_fir_tv_fc_fn(x=x,\n",
        "                                          L=L,\n",
        "                                          num_classes=num_classes,\n",
        "                                          time_filter_orders=time_filter_orders,\n",
        "                                          vertex_filter_orders=vertex_filter_orders,\n",
        "                                          num_filters=num_filters,\n",
        "                                          time_poolings=time_poolings,\n",
        "                                          vertex_poolings=vertex_poolings,\n",
        "                                          shot_noise=shot_noise)\n",
        "        dropout = tf.compat.v1.placeholder(tf.float32, name=\"keep_prob\")\n",
        "    elif model_type == \"deep_cheb\":\n",
        "        print(\"Training deep Chebyshev time invariant model...\")\n",
        "        xt = tf.transpose(x, perm=[0, 1, 3, 2])\n",
        "        logits, phase = deep_cheb_fc_fn(x=xt,\n",
        "                                        L=L,\n",
        "                                        num_classes=num_classes,\n",
        "                                        vertex_filter_orders=vertex_filter_orders,\n",
        "                                        num_filters=num_filters,\n",
        "                                        vertex_poolings=vertex_poolings,\n",
        "                                        shot_noise=shot_noise)\n",
        "        dropout = tf.compat.v1.placeholder(tf.float32, name=\"keep_prob\")\n",
        "    elif model_type == \"deep_sep\":\n",
        "        print(\"Training deep separable FIR model...\")\n",
        "        logits, phase = deep_sep_fir_fc_fn(x=x,\n",
        "                                           L=L,\n",
        "                                           num_classes=num_classes,\n",
        "                                           time_filter_orders=time_filter_orders,\n",
        "                                           vertex_filter_orders=vertex_filter_orders,\n",
        "                                           num_filters=num_filters,\n",
        "                                           time_poolings=time_poolings,\n",
        "                                           vertex_poolings=vertex_poolings)\n",
        "        dropout = tf.compat.v1.placeholder(tf.float32, name=\"keep_prob\")\n",
        "    elif model_type == \"fc\":\n",
        "        print(\"Training linear classifier model...\")\n",
        "        logits = fc_fn(x, num_classes)\n",
        "        dropout = tf.compat.v1.placeholder(tf.float32, name=\"keep_prob\")\n",
        "        phase = tf.compat.v1.placeholder(tf.bool, name=\"phase\")\n",
        "    else:\n",
        "        raise ValueError(\"model_type not valid.\")\n",
        "\n",
        "    # Define loss\n",
        "    with tf.name_scope(\"loss\"):\n",
        "        cross_entropy = tf.losses.softmax_cross_entropy(y_hot, logits=logits)\n",
        "        loss = tf.reduce_mean(cross_entropy)\n",
        "        tf.compat.v1.summary.scalar('xentropy', loss)\n",
        "\n",
        "        # Define metric\n",
        "    with tf.name_scope(\"metric\"):\n",
        "        correct_prediction = tf.equal(tf.argmax(logits, 1), tf.argmax(y_hot, 1))\n",
        "        correct_prediction = tf.cast(correct_prediction, tf.float32, name=\"correct_prediction\")\n",
        "        accuracy = tf.reduce_mean(correct_prediction, name=\"accuracy\")\n",
        "        tf.compat.v1.summary.scalar('accuracy', accuracy)\n",
        "\n",
        "    extra_update_ops = tf.compat.v1.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
        "    with tf.control_dependencies(extra_update_ops):\n",
        "        # Select optimizer\n",
        "        optimizer = tf.compat.v1.train.AdamOptimizer(learning_rate=learning_rate)\n",
        "        global_step = tf.Variable(0, name='global_step', trainable=False)\n",
        "        opt_train = optimizer.minimize(loss, global_step=global_step)\n",
        "\n",
        "    # Build the summary Tensor based on the TF collection of Summaries.\n",
        "    summary = tf.compat.v1.summary.merge_all()\n",
        "\n",
        "    # Create a saver for writing training checkpoints.\n",
        "    #saver = tf.train.Saver()\n",
        "\n",
        "    print(\"Number of training parameters:\", _number_of_trainable_params())\n",
        "\n",
        "    # Run session\n",
        "    with tf.Session() as sess:\n",
        "\n",
        "        # Instantiate a SummaryWriter to output summaries and the Graph.\n",
        "        #train_writer = tf.compat.v1.summary.FileWriter(log_dir + \"/train\", sess.graph)\n",
        "        #test_writer = tf.compat.v1.summary.FileWriter(log_dir + \"/test\", sess.graph)\n",
        "\n",
        "        sess.run(tf.global_variables_initializer())\n",
        "\n",
        "        MAX_STEPS = num_epochs * num_train // batch_size\n",
        "\n",
        "        # Start training loop\n",
        "        epoch_count = 0\n",
        "        for step in range(MAX_STEPS):\n",
        "\n",
        "            start_time = time.time()\n",
        "\n",
        "            feed_dict, _ = _fill_feed_dict(train_mb_source, x, y_, dropout, phase, True)\n",
        "\n",
        "            # Perform one training iteration\n",
        "            _, loss_value = sess.run([opt_train, loss],\n",
        "                                     feed_dict=feed_dict)\n",
        "\n",
        "            duration = time.time() - start_time\n",
        "\n",
        "            if step % (num_train // batch_size) == 0:\n",
        "                print(\"Epoch %d\" % epoch_count)\n",
        "                print(\"--------------------\")\n",
        "                epoch_count += 1\n",
        "\n",
        "            # Write the summaries and print an overview fairly often.\n",
        "            if step % 10 == 0:\n",
        "                # Print status to stdout.\n",
        "                accuracy_value = sess.run(accuracy, feed_dict=feed_dict)\n",
        "                print('Step %d: loss = %.2f accuracy = %.2f (%.3f sec)' % (step, loss_value, accuracy_value, duration))\n",
        "                # Update the events file.\n",
        "                #summary_str = sess.run(summary, feed_dict=feed_dict)\n",
        "                #train_writer.add_summary(summary_str, step)\n",
        "                #train_writer.flush()\n",
        "\n",
        "            # Save a checkpoint and evaluate the model periodically.\n",
        "            if (step + 1) % (num_train // batch_size) == 0 or (step + 1) == MAX_STEPS or (\n",
        "                    step + 1) % 30 == 0:\n",
        "                #checkpoint_file = os.path.join(log_dir, 'model')\n",
        "                #saver.save(sess, checkpoint_file, global_step=step)\n",
        "\n",
        "                test_accuracy = _eval_metric(sess, correct_prediction, dropout, phase, x, y_, test_mb_source)\n",
        "\n",
        "                #test_summary = tf.compat.v1.Summary(value=[tf.compat.v1.Summary.Value(tag=\"test_accuracy\", simple_value=test_accuracy)])\n",
        "                #test_writer.add_summary(test_summary, step)\n",
        "                #test_writer.flush()\n",
        "\n",
        "                print(\"--------------------\")\n",
        "                print('Test accuracy = %.2f' % test_accuracy)\n",
        "                if (step + 1) % (num_train // batch_size) == 0:\n",
        "                    print(\"====================\")\n",
        "                else:\n",
        "                    print(\"--------------------\")\n",
        "\n",
        "\n",
        "def _eval_metric(sess, correct_prediction, dropout, phase, x, y, test_mb_source, ):\n",
        "    still_data = True\n",
        "    test_correct_predictions = []\n",
        "    test_mb_source.restart()\n",
        "    while still_data:\n",
        "        test_feed_dict, still_data = _fill_feed_dict(test_mb_source, x, y, dropout, phase, False)\n",
        "        test_correct_predictions.append(sess.run(correct_prediction, feed_dict=test_feed_dict))\n",
        "\n",
        "    print(test_correct_predictions[1])\n",
        "    print(len(test_correct_predictions))\n",
        "    return np.mean(test_correct_predictions)\n",
        "\n",
        "\n",
        "def run_eval(test_mb_source):\n",
        "    with tf.Session() as sess:\n",
        "        #saver = tf.train.import_meta_graph(\n",
        "          #  os.path.join(log_dir, \"model-\" + str(_last_checkpoint(log_dir)) + \".meta\"))\n",
        "        #saver.restore(sess, tf.train.latest_checkpoint(log_dir))\n",
        "        graph = tf.get_default_graph()\n",
        "\n",
        "        # Get inputs\n",
        "        x = graph.get_tensor_by_name(\"x:0\")\n",
        "        y = graph.get_tensor_by_name(\"labels:0\")\n",
        "        keep_prob = graph.get_tensor_by_name(\"keep_prob:0\")\n",
        "        phase = graph.get_tensor_by_name(\"phase:0\")\n",
        "\n",
        "        # Get output\n",
        "        correct_prediction = graph.get_tensor_by_name(\"metric/correct_prediction:0\")\n",
        "\n",
        "        print(\"Evaluation accuracy: %.2f\" % _eval_metric(sess, correct_prediction, keep_prob, phase, x, y,\n",
        "                                                         test_mb_source))\n",
        "\n",
        "        for idx, v in enumerate([v for v in tf.trainable_variables() if \"conv\" in v.name]):\n",
        "            plot_tf_fir_filter(sess, v, os.path.join(log_dir, \"conv_%d\" % idx))\n",
        "\n",
        "\n",
        "\n",
        "def _number_of_pooling_levels(vertex_poolings):\n",
        "    return np.log2(np.prod(vertex_poolings)).astype(int)\n",
        "\n",
        "\n",
        "def _number_of_trainable_params():\n",
        "    return np.sum([np.product(x.shape) for x in tf.trainable_variables()])\n",
        "\n",
        "\n",
        "def _last_exp(log_dir):\n",
        "    exp_numbers = []\n",
        "    if not os.path.exists(log_dir):\n",
        "        return 0\n",
        "    for file in os.listdir(log_dir):\n",
        "        if \"exp\" not in file:\n",
        "            continue\n",
        "        else:\n",
        "            exp_numbers.append(int(file.split(\"_\")[1]))\n",
        "    return max(exp_numbers) if len(exp_numbers) > 0 else 0\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rB-FN8-pnNB0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hNH5rKGObjwh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def main():\n",
        "    # Initialize tempdir\n",
        "\n",
        "    if action == \"train\":\n",
        "\n",
        "        # Initialize data\n",
        "        G = graphs.Community(num_vertices, seed=seed)\n",
        "        G.compute_laplacian(\"normalized\")\n",
        "        \n",
        "        # Prepare pooling\n",
        "        num_levels = _number_of_pooling_levels(vertex_poolings)\n",
        "        error = True\n",
        "        while error:\n",
        "            try:\n",
        "                adjacencies, perm = coarsen(G.A, levels=num_levels)  # Coarsens in powers of 2\n",
        "                error = False\n",
        "            except IndexError:\n",
        "                error = True\n",
        "                continue\n",
        "\n",
        "        L = [initialize_laplacian_tensor(A) for A in adjacencies]\n",
        "        L = keep_pooling_laplacians(L, vertex_poolings)\n",
        "\n",
        "    elif action == \"eval\":\n",
        "        W = np.load(os.path.join(log_dir, \"graph_weights.npy\"))\n",
        "        G = graphs.Graph(W)\n",
        "        G.compute_laplacian(\"normalized\")\n",
        "\n",
        "    if action == \"train\":\n",
        "        #G_nx = nx.from_numpy_matrix(G.W.todense())\n",
        "        if load_data:\n",
        "            #num test can't exceed saved array size\n",
        "            train_data, train_labels = load_from_npy('train',num_train)\n",
        "\n",
        "        else:     \n",
        "            random.seed(seed)\n",
        "            train_data, train_labels = epidemics_generator(\n",
        "                g_nx = nx.from_numpy_matrix(G.W.todense()),\n",
        "                batch_size = num_train,\n",
        "                timesteps=num_frames,\n",
        "                initial_nodes = random.sample(range(num_vertices), int(num_vertices/10))\n",
        "            )\n",
        "            save_to_npy('train',train_data, train_labels)\n",
        "\n",
        "        train_data = perm_data(train_data, perm)\n",
        "        train_mb_source = MinibatchSource(train_data, train_labels, repeat=True)\n",
        "\n",
        "    if load_data:\n",
        "            #num test can't exceed saved array size\n",
        "            test_data, test_labels = load_from_npy('test',num_test)\n",
        "           \n",
        "    else:     \n",
        "        test_data, test_labels = epidemics_generator(\n",
        "                g_nx = nx.from_numpy_matrix(G.W.todense()),\n",
        "                batch_size = num_test,\n",
        "                timesteps=num_frames,\n",
        "                initial_nodes = random.sample(range(num_vertices), int(num_vertices/10))\n",
        "            )\n",
        "        save_to_npy('test',test_data, test_labels)\n",
        "\n",
        "    test_data = perm_data(test_data, perm)\n",
        "    test_mb_source = MinibatchSource(test_data, test_labels, repeat=False)\n",
        "\n",
        "    if action == \"train\":\n",
        "        #params = vars(FLAGS)\n",
        "        #with open(os.path.join(log_dir, \"params.json\"), \"w\") as f:\n",
        "        #    json.dump(params, f)\n",
        "\n",
        "        # Run training and evaluation loop\n",
        "        print(\"Training model...\")\n",
        "        run_training(L, train_mb_source, test_mb_source)\n",
        "    elif action == \"eval\":\n",
        "        print(\"Evaluating model...\")\n",
        "        run_eval(test_mb_source)\n",
        "    else:\n",
        "        raise ValueError(\"No valid action selected\")\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ws0Xb3ZVb65V",
        "colab_type": "code",
        "outputId": "e75770ee-19d3-4add-c09e-1b7a1c4169ee",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "main()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2019-10-16 22:29:55,642:[INFO](pygsp.graphs.community.__init__): Constructed using eps-NN with eps = 3.34370152488211\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Layer 0: M_0 = |V| = 536 nodes (36 added),|E| = 3686 edges\n",
            "Layer 1: M_1 = |V| = 268 nodes (9 added),|E| = 1890 edges\n",
            "Layer 2: M_2 = |V| = 134 nodes (3 added),|E| = 863 edges\n",
            "Layer 3: M_3 = |V| = 67 nodes (0 added),|E| = 473 edges\n",
            "[1.6408342]\n",
            "[1.549751]\n",
            "[1.4593617]\n",
            "[1.3968053]\n",
            "\n",
            "Generating epidemics\n",
            "\n",
            "\n",
            "6000\n",
            "\n",
            " \n",
            " Epidemics Data Generated. Shape: (6000, 500, 128, 1)\n",
            "(6000,)\n",
            "\n",
            "Generating epidemics\n",
            "\n",
            "\n",
            "2000\n",
            "\n",
            " \n",
            " Epidemics Data Generated. Shape: (2000, 500, 128, 1)\n",
            "(2000,)\n",
            "Training model...\n",
            "Training deep FIR-TV model...\n",
            "WARNING:tensorflow:From /content/models.py:67: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "WARNING:tensorflow:From /content/models.py:10: The name tf.truncated_normal is deprecated. Please use tf.random.truncated_normal instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/util/deprecation.py:574: calling conv1d (from tensorflow.python.ops.nn_ops) with data_format=NHWC is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "`NHWC` for data_format is deprecated, use `NWC` instead\n",
            "WARNING:tensorflow:From /content/layers.py:235: The name tf.sparse_tensor_dense_matmul is deprecated. Please use tf.sparse.sparse_dense_matmul instead.\n",
            "\n",
            "WARNING:tensorflow:\n",
            "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "  * https://github.com/tensorflow/io (for I/O related ops)\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n",
            "WARNING:tensorflow:From /content/models.py:80: max_pooling2d (from tensorflow.python.layers.pooling) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.MaxPooling2D instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/layers/pooling.py:311: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `layer.__call__` method instead.\n",
            "WARNING:tensorflow:From /content/models.py:95: flatten (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.flatten instead.\n",
            "WARNING:tensorflow:From /content/models.py:100: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.Dense instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/losses/losses_impl.py:121: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "Number of training parameters: 16354\n",
            "Epoch 0\n",
            "--------------------\n",
            "Step 0: loss = 1.12 accuracy = 0.66 (5.132 sec)\n",
            "Step 10: loss = 0.38 accuracy = 0.88 (1.253 sec)\n",
            "Step 20: loss = 0.25 accuracy = 0.93 (1.229 sec)\n",
            "[1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1.]\n",
            "20\n",
            "--------------------\n",
            "Test accuracy = 0.94\n",
            "--------------------\n",
            "Step 30: loss = 0.14 accuracy = 0.97 (1.220 sec)\n",
            "Step 40: loss = 0.16 accuracy = 0.96 (1.216 sec)\n",
            "Step 50: loss = 0.16 accuracy = 0.97 (1.227 sec)\n",
            "[1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1.]\n",
            "20\n",
            "--------------------\n",
            "Test accuracy = 0.94\n",
            "====================\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xHON5do_hDJZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}